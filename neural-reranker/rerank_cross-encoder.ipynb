{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50bd4c5-c362-40b8-a0fd-c99caf92cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TOP_K = 20\n",
    "FINAL_K = 5\n",
    "\n",
    "df_collection = pd.read_pickle(\"subtask4b_collection_data.pkl\")\n",
    "df_query = pd.read_csv(\"subtask4b_query_tweets_dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_collection[\"full_text\"] = df_collection[\"title\"].fillna('') + \" \" + df_collection[\"abstract\"].fillna('')\n",
    "doc_texts = df_collection[\"full_text\"].tolist()\n",
    "doc_uids = df_collection[\"cord_uid\"].tolist()\n",
    "\n",
    "queries = df_query[\"tweet_text\"].tolist()\n",
    "query_ids = df_query[\"post_id\"].tolist()\n",
    "true_labels = df_query[\"cord_uid\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0a4352-e49e-4967-9ffe-4fa3e1ecdbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding with SBERT for candidate retrieval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e9d09c5bc94d779b521fef0982dd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33870af8ec3f4c3491a0a5ffc0298a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Encoding with SBERT for candidate retrieval\")\n",
    "sbert_model = SentenceTransformer(\"fine-tuned-multi-qa-MiniLM-L6-cos-v1\")\n",
    "doc_embeds = sbert_model.encode(doc_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "query_embeds = sbert_model.encode(queries, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "topk_candidates = {}\n",
    "for i, qvec in enumerate(query_embeds):\n",
    "    scores = util.cos_sim(qvec, doc_embeds)[0]\n",
    "    top_k = torch.topk(scores, k=TOP_K)\n",
    "    indices = top_k.indices.tolist()\n",
    "    topk_candidates[query_ids[i]] = [(doc_uids[j], doc_texts[j]) for j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3269044c-8fd5-41f4-9876-338e0c6428ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Reranker: cross-msmarco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1400/1400 [49:12<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-msmarco MRR@5: 0.6401\n",
      "\n",
      "Final Reranker MRR@5 Scores:\n",
      "===================================\n",
      "cross-msmarco        MRR@5: 0.6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#rerankers to compare\n",
    "reranker_models = {\n",
    "    \"cross-msmarco\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    #\"modernbert-base\": \"answerdotai/ModernBERT-base\",\n",
    "    #\"tinybert-stsb\": \"cross-encoder/stsb-TinyBERT-L4\",\n",
    "    #\"bge-reranker-v2\": \"BAAI/bge-reranker-v2-m3\",\n",
    "}\n",
    "\n",
    "def mrr_at_k(predictions, k=5):\n",
    "    total = 0.0\n",
    "    for pred in predictions:\n",
    "        if pred[\"true\"] in pred[\"preds\"]:\n",
    "            rank = pred[\"preds\"].index(pred[\"true\"]) + 1\n",
    "            total += 1 / rank\n",
    "    return total / len(predictions)\n",
    "\n",
    "#evaluate reranker\n",
    "results_summary = {}\n",
    "\n",
    "for model_name, model_path in reranker_models.items():\n",
    "    print(f\"\\nEvaluating Reranker: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, qid in enumerate(tqdm(query_ids)):\n",
    "            query = queries[i]\n",
    "            candidates = topk_candidates[qid]\n",
    "            inputs = tokenizer(\n",
    "                [query] * len(candidates),\n",
    "                [text for _, text in candidates],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits.squeeze().cpu().tolist()\n",
    "\n",
    "            ranked = sorted(zip(candidates, logits), key=lambda x: x[1], reverse=True)\n",
    "            top_preds = [doc_id for (doc_id, _), _ in ranked[:FINAL_K]]\n",
    "\n",
    "            predictions.append({\n",
    "                \"post_id\": qid,\n",
    "                \"true\": true_labels[i],\n",
    "                \"preds\": top_preds\n",
    "            })\n",
    "\n",
    "        mrr = mrr_at_k(predictions, k=FINAL_K)\n",
    "\n",
    "        df_preds = pd.DataFrame([\n",
    "            {\"post_id\": pred[\"post_id\"], \"preds\": pred[\"preds\"]}\n",
    "            for pred in predictions\n",
    "        ])\n",
    "\n",
    "        df_preds.to_csv(f\"{model_name}_predictions.tsv\", sep=\"\\t\", index=False)\n",
    "        results_summary[model_name] = round(mrr, 4)\n",
    "        print(f\"{model_name} MRR@5: {mrr:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {model_name}: {e}\")\n",
    "\n",
    "print(\"\\nFinal Reranker MRR@5 Scores:\")\n",
    "print(\"=\" * 35)\n",
    "for model, score in results_summary.items():\n",
    "    print(f\"{model:<20} MRR@5: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c3d3ffa-be71-41bd-8ea7-de58f47ac0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Reranker: modernbert-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279575617bc84a7ca610faec597e0f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3dfe7ef835b443ebdfcb615dba66836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748a31b50b7d47129d16888233ffaac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87601a599d5c46ed9f64db15a5c9538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1528759508484263b22b99f25f647765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████| 1400/1400 [10:30:03<00:00, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modernbert-base MRR@5: 0.0978\n",
      "\n",
      "Final Reranker MRR@5 Scores:\n",
      "===================================\n",
      "cross-msmarco        MRR@5: 0.6401\n",
      "modernbert-base      MRR@5: 0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-11:\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-12:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-2:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-7:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-8:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-5:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-6:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Process ForkProcess-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 244, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#rerankers to compare\n",
    "reranker_models = {\n",
    "    #\"cross-msmarco\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    \"modernbert-base\": \"answerdotai/ModernBERT-base\",\n",
    "    #\"tinybert-stsb\": \"cross-encoder/stsb-TinyBERT-L4\",\n",
    "    #\"bge-reranker-v2\": \"BAAI/bge-reranker-v2-m3\",\n",
    "}\n",
    "\n",
    "def mrr_at_k(predictions, k=5):\n",
    "    total = 0.0\n",
    "    for pred in predictions:\n",
    "        if pred[\"true\"] in pred[\"preds\"]:\n",
    "            rank = pred[\"preds\"].index(pred[\"true\"]) + 1\n",
    "            total += 1 / rank\n",
    "    return total / len(predictions)\n",
    "\n",
    "#evaluate reranker\n",
    "#results_summary = {}\n",
    "\n",
    "for model_name, model_path in reranker_models.items():\n",
    "    print(f\"\\nEvaluating Reranker: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, qid in enumerate(tqdm(query_ids)):\n",
    "            query = queries[i]\n",
    "            candidates = topk_candidates[qid]\n",
    "            inputs = tokenizer(\n",
    "                [query] * len(candidates),\n",
    "                [text for _, text in candidates],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits.squeeze().cpu().tolist()\n",
    "\n",
    "            ranked = sorted(zip(candidates, logits), key=lambda x: x[1], reverse=True)\n",
    "            top_preds = [doc_id for (doc_id, _), _ in ranked[:FINAL_K]]\n",
    "\n",
    "            predictions.append({\n",
    "                \"post_id\": qid,\n",
    "                \"true\": true_labels[i],\n",
    "                \"preds\": top_preds\n",
    "            })\n",
    "\n",
    "        mrr = mrr_at_k(predictions, k=FINAL_K)\n",
    "\n",
    "        df_preds = pd.DataFrame([\n",
    "            {\"post_id\": pred[\"post_id\"], \"preds\": pred[\"preds\"]}\n",
    "            for pred in predictions\n",
    "        ])\n",
    "\n",
    "        df_preds.to_csv(f\"{model_name}_predictions.tsv\", sep=\"\\t\", index=False)\n",
    "        results_summary[model_name] = round(mrr, 4)\n",
    "        print(f\"{model_name} MRR@5: {mrr:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {model_name}: {e}\")\n",
    "\n",
    "print(\"\\nFinal Reranker MRR@5 Scores:\")\n",
    "print(\"=\" * 35)\n",
    "for model, score in results_summary.items():\n",
    "    print(f\"{model:<20} MRR@5: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b73b44a-de41-42ad-a066-ed1b5a70d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Reranker: tinybert-stsb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbd239604df4a2694d76b3e535ddcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc5bba80e1249dda38f6a5b348d45e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14be6d6740d746fbbe1955f7c17955e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3adf0217134c6689a045e7b52beab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5f0e41bb2541aa885ab6a35248a359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba04a119cd0e4fdf80ad79bf167a555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/57.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1400/1400 [30:46<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinybert-stsb MRR@5: 0.1683\n",
      "\n",
      "Final Reranker MRR@5 Scores:\n",
      "===================================\n",
      "cross-msmarco        MRR@5: 0.6401\n",
      "modernbert-base      MRR@5: 0.0978\n",
      "tinybert-stsb        MRR@5: 0.1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#rerankers to compare\n",
    "reranker_models = {\n",
    "    #\"cross-msmarco\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    #\"modernbert-base\": \"answerdotai/ModernBERT-base\",\n",
    "    \"tinybert-stsb\": \"cross-encoder/stsb-TinyBERT-L4\",\n",
    "    #\"bge-reranker-v2\": \"BAAI/bge-reranker-v2-m3\",\n",
    "}\n",
    "\n",
    "def mrr_at_k(predictions, k=5):\n",
    "    total = 0.0\n",
    "    for pred in predictions:\n",
    "        if pred[\"true\"] in pred[\"preds\"]:\n",
    "            rank = pred[\"preds\"].index(pred[\"true\"]) + 1\n",
    "            total += 1 / rank\n",
    "    return total / len(predictions)\n",
    "\n",
    "#evaluate reranker\n",
    "#results_summary = {}\n",
    "\n",
    "for model_name, model_path in reranker_models.items():\n",
    "    print(f\"\\nEvaluating Reranker: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, qid in enumerate(tqdm(query_ids)):\n",
    "            query = queries[i]\n",
    "            candidates = topk_candidates[qid]\n",
    "            inputs = tokenizer(\n",
    "                [query] * len(candidates),\n",
    "                [text for _, text in candidates],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits.squeeze().cpu().tolist()\n",
    "\n",
    "            ranked = sorted(zip(candidates, logits), key=lambda x: x[1], reverse=True)\n",
    "            top_preds = [doc_id for (doc_id, _), _ in ranked[:FINAL_K]]\n",
    "\n",
    "            predictions.append({\n",
    "                \"post_id\": qid,\n",
    "                \"true\": true_labels[i],\n",
    "                \"preds\": top_preds\n",
    "            })\n",
    "\n",
    "        mrr = mrr_at_k(predictions, k=FINAL_K)\n",
    "\n",
    "        df_preds = pd.DataFrame([\n",
    "            {\"post_id\": pred[\"post_id\"], \"preds\": pred[\"preds\"]}\n",
    "            for pred in predictions\n",
    "        ])\n",
    "\n",
    "        df_preds.to_csv(f\"{model_name}_predictions.tsv\", sep=\"\\t\", index=False)\n",
    "        results_summary[model_name] = round(mrr, 4)\n",
    "        print(f\"{model_name} MRR@5: {mrr:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {model_name}: {e}\")\n",
    "\n",
    "print(\"\\nFinal Reranker MRR@5 Scores:\")\n",
    "print(\"=\" * 35)\n",
    "for model, score in results_summary.items():\n",
    "    print(f\"{model:<20} MRR@5: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd2df70-ce94-4aeb-a602-7a06477bcde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Reranker: nli-deberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8650619052164bc39375522771d90cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bc334ceba6492d859b9da8f24a7787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97a655b7b0b427dbbede6310d70d762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbb8ed631374e118f58f5470534df66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef03e32058cd4d5bb4ad9cffaefe1667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a78936c38c443a4a6c438b6527cd3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eeeebe3e5c42f6abe5404b2898ac90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/738M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1400/1400 [6:17:50<00:00, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nli-deberta MRR@5: 0.0731\n",
      "\n",
      "Final Reranker MRR@5 Scores:\n",
      "===================================\n",
      "cross-msmarco        MRR@5: 0.6401\n",
      "modernbert-base      MRR@5: 0.0978\n",
      "tinybert-stsb        MRR@5: 0.1683\n",
      "nli-deberta          MRR@5: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#rerankers to compare\n",
    "reranker_models = {\n",
    "    #\"cross-msmarco\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    #\"modernbert-base\": \"answerdotai/ModernBERT-base\",\n",
    "    #\"tinybert-stsb\": \"cross-encoder/stsb-TinyBERT-L4\",\n",
    "    #\"bge-reranker-v2\": \"BAAI/bge-reranker-v2-m3\",\n",
    "    \"nli-deberta\" : \"cross-encoder/nli-deberta-v3-base\"\n",
    "}\n",
    "\n",
    "def mrr_at_k(predictions, k=5):\n",
    "    total = 0.0\n",
    "    for pred in predictions:\n",
    "        if pred[\"true\"] in pred[\"preds\"]:\n",
    "            rank = pred[\"preds\"].index(pred[\"true\"]) + 1\n",
    "            total += 1 / rank\n",
    "    return total / len(predictions)\n",
    "\n",
    "#evaluate reranker\n",
    "#results_summary = {}\n",
    "\n",
    "for model_name, model_path in reranker_models.items():\n",
    "    print(f\"\\nEvaluating Reranker: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, qid in enumerate(tqdm(query_ids)):\n",
    "            query = queries[i]\n",
    "            candidates = topk_candidates[qid]\n",
    "            inputs = tokenizer(\n",
    "                [query] * len(candidates),\n",
    "                [text for _, text in candidates],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits.squeeze().cpu().tolist()\n",
    "\n",
    "            ranked = sorted(zip(candidates, logits), key=lambda x: x[1], reverse=True)\n",
    "            top_preds = [doc_id for (doc_id, _), _ in ranked[:FINAL_K]]\n",
    "\n",
    "            predictions.append({\n",
    "                \"post_id\": qid,\n",
    "                \"true\": true_labels[i],\n",
    "                \"preds\": top_preds\n",
    "            })\n",
    "\n",
    "        mrr = mrr_at_k(predictions, k=FINAL_K)\n",
    "\n",
    "        df_preds = pd.DataFrame([\n",
    "            {\"post_id\": pred[\"post_id\"], \"preds\": pred[\"preds\"]}\n",
    "            for pred in predictions\n",
    "        ])\n",
    "\n",
    "        df_preds.to_csv(f\"{model_name}_predictions.tsv\", sep=\"\\t\", index=False)\n",
    "        results_summary[model_name] = round(mrr, 4)\n",
    "        print(f\"{model_name} MRR@5: {mrr:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {model_name}: {e}\")\n",
    "\n",
    "print(\"\\nFinal Reranker MRR@5 Scores:\")\n",
    "print(\"=\" * 35)\n",
    "for model, score in results_summary.items():\n",
    "    print(f\"{model:<20} MRR@5: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
